---
title: "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting"
collection: publications
category: conferences
permalink: /publication/2025-wave
date: 2025-07-19
venue: "ICML 2025 (Poster), PMLR 267: 40464–40490"
authors: "Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang"
paperurl: "https://proceedings.mlr.press/v267/lu25d.html"
paperpdf: "https://raw.githubusercontent.com/mlresearch/v267/main/assets/lu25d/lu25d.pdf"
code: "https://github.com/LJC-FVNR/ARMA-Attention"
selected: true
excerpt: "Adds ARMA structure to autoregressive attention via a weighted varying gate, decoupling long‑range and local effects and improving TSF quality without increasing asymptotic complexity."
tags: [Time Series Forecasting, ARMA, Linear Attention, Transformers]
# citation: "Lu, J., Han, X., Sun, Y., & Yang, S. (2025). WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting. In ICML 2025, PMLR 267: 40464–40490."
---
Demonstrates that properly tokenized autoregressive decoder‑only Transformers are strong TSF baselines and that introducing AR and MA components through WAVE yields consistent accuracy gains across efficient attentions.