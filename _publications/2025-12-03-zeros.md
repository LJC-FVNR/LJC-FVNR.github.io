---
title: "ZeroS: Zero‑Sum Linear Attention for Efficient Transformers"
collection: publications
permalink: /publication/2025-zeros
category: conferences
date: 2025-12-03
venue: "NeurIPS 2025 (Spotlight)"
authors: "Jiecheng Lu, Xu Han, Yan Sun, Viresh Pati, Yubin Kim, Siddhartha Somani, Shihao Yang"
paperurl: "https://neurips.cc/virtual/2025/poster/118425"
paperpdf: "https://openreview.net/pdf/53b99866f7a487c410012b2077f3c4dc78c72742.pdf"
code: "https://github.com/LJC-FVNR/SequenceLab"
selected: true
excerpt: "Introduces Zero‑Sum Linear Attention (ZeroS), which removes the uniform zero‑order term and reweights residuals to enable stable positive/negative attention weights,  allowing contrastive operations within a single layer while retaining O(N) complexity."
tags: [Transformers, Linear Attention, Sequence Modeling]
# citation: "Lu, J., Han, X., Sun, Y., Pati, V., Kim, Y., Somani, S., & Yang, S. (2025). ZeroS: Zero‑Sum Linear Attention for Efficient Transformers. NeurIPS 2025."
---
A zero‑sum reparameterization of linear attention that avoids uniform accumulation bias and allows contrastive operations within a single layer, narrowing the gap with softmax attention under linear complexity.