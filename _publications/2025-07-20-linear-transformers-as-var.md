---
title: "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting"
collection: publications
category: conferences
permalink: /publication/2025-linear-transformers-as-var
date: 2025-07-20
venue: "ICML 2025 (Poster), PMLR 267: 40848–40867"
authors: "Jiecheng Lu, Shihao Yang"
paperurl: "https://proceedings.mlr.press/v267/lu25r.html"
paperpdf: "https://raw.githubusercontent.com/mlresearch/v267/main/assets/lu25r/lu25r.pdf"
code: "https://github.com/LJC-FVNR/Structural-Aligned-Mixture-of-VAR"
selected: true
excerpt: "Shows that a linear attention layer can be interpreted as a dynamic VAR; proposes SAMoVAR to realign multi‑layer Transformers with autoregressive forecasting for improved interpretability and accuracy."
tags: [Time Series Forecasting, Linear Attention, Vector Autoregression]
# citation: "Lu, J. & Yang, S. (2025). Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting. In ICML 2025, PMLR 267: 40848–40867."
---
Interprets linear attention through the lens of VAR and reorganizes the stack (attention/MLP/I‑O flow) to match autoregressive objectives, yielding more interpretable and competitive multivariate TSF models.